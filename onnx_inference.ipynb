{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onnx_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1o9WwU9LWpbYdKPpiFe6lQyY5I2uB1LNC",
      "authorship_tag": "ABX9TyPPhXhBIMjuYkwOt8jsTxrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hannes1/youtube-transcript-api/blob/master/onnx_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAg5KBfyYLj9"
      },
      "source": [
        "## Install NeMo\n",
        "BRANCH = 'v1.0.0b2'\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs\n",
        "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/config.yaml\n",
        "!pip install onnxruntime\n",
        "!python -m pip install 'git+https://github.com/NVIDIA/NeMo.git@v1.0.0b2#egg=nemo_toolkit[all]'\n",
        "!git clone https://Hannes1:Hansie13!@github.com/Hannes1/chatable_asr_data_downloader.git\n",
        "!pip install nemo-toolkit[nlp]==1.0.0b1\n",
        "!pip install gdown #Get onnx model\n",
        "!gdown https://drive.google.com/uc?id=18IgsCFS7fwlv18FGAhDAfnETFPOw3Mo7\n",
        "%cd ./chatable_asr_data_downloader\n",
        "!python -m pip install git+https://github.com/nficano/pytube\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akPcsypLeI7I"
      },
      "source": [
        "#Just to get audio file move and rename wav file to test in main directory\n",
        "#Can just upload own voice and call test.wav\n",
        "from download_audio import download_audio\n",
        "from pytube import YouTube\n",
        "from cut_audio import cut_audio,format_to_wav\n",
        "id = \"BtN-goy9VOY\" #Youtube Id\n",
        "download_audio(id,'../')\n",
        "format_to_wav(id,'../', \"mp4\", 1, 16000)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQCbfSd4WjZX"
      },
      "source": [
        "%cd ../content\n",
        "%mv BtN-goy9VOY.wav test.wav #for test purposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwEWrklukTx"
      },
      "source": [
        "def cut_text(text,max_length):\n",
        "    sentence_array = []\n",
        "    current_character = 1\n",
        "    string_length = len(text)\n",
        "    forward_count = 0\n",
        "    character_count = 0\n",
        "    while character_count < string_length:\n",
        "        sentence = text[current_character-1:current_character + forward_count +\n",
        "                            max_length-1]  # cut string -1 because the first word doesn't have a space so it's lost\n",
        "        if sentence.endswith(' '):\n",
        "            sentence_array.append(sentence)\n",
        "            current_character = current_character + 128 + forward_count\n",
        "            character_count += 128\n",
        "            forward_count = 0\n",
        "        else:\n",
        "            character_count += 1\n",
        "            forward_count += 1\n",
        "\n",
        "    return sentence_array        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_twIx8IbX0OR"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "import onnxruntime\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.collections.asr.data.audio_to_text import AudioToCharDataset\n",
        "from nemo.collections.asr.metrics.wer import WER\n",
        "from ruamel.yaml import YAML\n",
        "import nemo.collections.nlp as nemo_nlp\n",
        "\n",
        "nlp_model = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(model_name=\"Punctuation_Capitalization_with_DistilBERT\")\n",
        "\n",
        "quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(\n",
        "    model_name=\"QuartzNet15x5Base-En\")\n",
        "\n",
        "config_path = './configs/config.yaml'\n",
        "\n",
        "\n",
        "yaml = YAML(typ='safe')\n",
        "with open(config_path) as f:\n",
        "    params = yaml.load(f)\n",
        "print(params)\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() #conver tensor to numpy.ndarray\n",
        "\n",
        "\n",
        "def setup_transcribe_dataloader(cfg, vocabulary):\n",
        "    config = {\n",
        "        'manifest_filepath': os.path.join(cfg['temp_dir'], 'manifest.json'),\n",
        "        'sample_rate': 16000,\n",
        "        'labels': vocabulary, \n",
        "        'batch_size': min(cfg['batch_size'], len(cfg['paths2audio_files'])),\n",
        "        'trim_silence': True,\n",
        "        'shuffle': False,\n",
        "    }\n",
        "    dataset = AudioToCharDataset(\n",
        "        manifest_filepath=config['manifest_filepath'],\n",
        "        labels=config['labels'],\n",
        "        sample_rate=config['sample_rate'],\n",
        "        int_values=config.get('int_values', False),\n",
        "        augmentor=None,\n",
        "        max_duration=config.get('max_duration', None),\n",
        "        min_duration=config.get('min_duration', None),\n",
        "        max_utts=config.get('max_utts', 0),\n",
        "        blank_index=config.get('blank_index', -1),\n",
        "        unk_index=config.get('unk_index', -1),\n",
        "        normalize=config.get('normalize_transcripts', False),\n",
        "        trim=config.get('trim_silence', True),\n",
        "        load_audio=config.get('load_audio', True),\n",
        "        parser=config.get('parser', 'en'),\n",
        "        add_misc=config.get('add_misc', False),\n",
        "    )\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        collate_fn=dataset.collate_fn,\n",
        "        drop_last=config.get('drop_last', False),\n",
        "        shuffle=False,\n",
        "        num_workers=config.get('num_workers', 0),\n",
        "        pin_memory=config.get('pin_memory', False),\n",
        "    )\n",
        "\n",
        "# quartznet.export('qn.onnx')\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession('./qn.onnx')\n",
        "vocabulary = [\" \", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\",\n",
        "              \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"'\"]\n",
        "\n",
        "files = ['./test.wav']\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    with open(os.path.join(tmpdir, 'manifest.json'), 'w') as fp:\n",
        "        for audio_file in files:\n",
        "            entry = {'audio_filepath': audio_file,\n",
        "                     'duration': 100000, 'text': 'nothing'}\n",
        "            fp.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "    config = {'paths2audio_files': './test.wav',\n",
        "              'batch_size': 4, 'temp_dir': tmpdir}\n",
        "    temporary_datalayer = setup_transcribe_dataloader(\n",
        "        config, vocabulary)\n",
        "    for test_batch in temporary_datalayer:\n",
        "        processed_signal, processed_signal_len = quartznet.preprocessor(\n",
        "            input_signal=test_batch[0].to('cpu'), length=test_batch[1].to('cpu')\n",
        "        )\n",
        "        ort_inputs = {ort_session.get_inputs(\n",
        "        )[0].name: to_numpy(processed_signal), }\n",
        "        ologits = ort_session.run(None, ort_inputs)\n",
        "        alogits = np.asarray(ologits)\n",
        "        logits = torch.from_numpy(alogits[0])\n",
        "        greedy_predictions = logits.argmax(dim=-1, keepdim=False)\n",
        "        wer = WER(vocabulary=vocabulary,\n",
        "                  batch_dim_index=0, use_cer=False, ctc_decode=True)\n",
        "        hypotheses = wer.ctc_decoder_predictions_tensor(greedy_predictions)\n",
        "        print(hypotheses)\n",
        "        string = hypotheses[0]\n",
        "        queries = cut_text(string,128)\n",
        "        inference_results = nlp_model.add_punctuation_capitalization(queries)\n",
        "        for query, result in zip(queries, inference_results):\n",
        "          print(f'Query : {query}')\n",
        "          print(f'Result: {result.strip()}\\n')\n",
        "        # print(result)\n",
        "        break\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clx58kTsaw2E"
      },
      "source": [
        "# Todo\n",
        "\n",
        "1.   Figure out how to convert nlp model to onnx\n",
        "2.   Fine tune the punctuation model with more data and bigger input of characters\n",
        "3.   Convert to flask \n",
        "\n"
      ]
    }
  ]
}